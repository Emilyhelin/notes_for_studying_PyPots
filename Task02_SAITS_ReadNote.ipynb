{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052b281e",
   "metadata": {},
   "source": [
    "# 📘 SAITS: Self-Attention-based Imputation for Time Series 阅读笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8ce98",
   "metadata": {},
   "source": [
    "## 1️⃣ 文章基本框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6341b",
   "metadata": {},
   "source": [
    "### 背景以及核心相关文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6d662",
   "metadata": {},
   "source": [
    "| 序号 | 论文标题                                        | 使用模型                               |\n",
    "| -- | ------------------------------------------- | ---------------------------------- |\n",
    "| 1  | BRITS: Bidirectional RNN Imputation         | Bi-RNN                             |\n",
    "| 2  | GP-VAE: Gaussian Process VAE                | VAE + GP                           |\n",
    "| 3  | NRTSI: Non-Recurrent Time Series Imputation | Transformer Encoder + Nested Loops |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0318c",
   "metadata": {},
   "source": [
    "\n",
    "### 目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec47e10",
   "metadata": {},
   "source": [
    "提出一种高效、非递归的自注意力机制模型 SAITS，用于多变量时间序列的缺失值填补（imputation）。目标是提高填补精度、加快训练速度、减少对复杂假设的依赖，同时支持在真实世界数据中有效运用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d13733",
   "metadata": {},
   "source": [
    "### 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7d852",
   "metadata": {},
   "source": [
    "*   SAITS 显著优于现有主流方法（如 BRITS、Transformer）在多个公开数据集上取得 SOTA。\n",
    "*   相较 Transformer，SAITS 参数更少、计算更快，结构更高效。\n",
    "*   SAITS 提出的联合训练方法（Joint-Optimization of Imputation and Reconstruction）适用于多种模型，不局限于 SAITS。\n",
    "*   在下游任务如分类上，SAITS 的填补提升了性能，验证其实用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe347b92",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3bf26",
   "metadata": {},
   "source": [
    "## 2️⃣ 结果与讨论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92de66",
   "metadata": {},
   "source": [
    "### 数据以及数据来源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352a333",
   "metadata": {},
   "source": [
    "| 数据集名称          | 来源 & 特点                      |\n",
    "| -------------- | ---------------------------- |\n",
    "| PhysioNet-2012 | ICU 病人数据，极度稀疏（80% 缺失）        |\n",
    "| Air-Quality    | 北京12个监测点的空气质量数据，轻微缺失（1.6%）   |\n",
    "| Electricity    | 370个客户的用电数据（无缺失），人为设置缺失率进行对比 |\n",
    "| ETT            | 变压器温度数据，无缺失，滑窗采样构建样本         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de49a41",
   "metadata": {},
   "source": [
    "\n",
    "### 实施"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b805a4",
   "metadata": {},
   "source": [
    "![\\<img alt=\"\" data-attachment-key=\"C8HTQM3C\" width=\"973\" height=\"378\" src=\"attachments/C8HTQM3C.png\" ztype=\"zimage\"> | 973](attachments/C8HTQM3C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22140bf5",
   "metadata": {},
   "source": [
    "输入带有**自然缺失**的数据X，然后经过随机掩码，制造**人工缺失**。之后使用两种掩码，分别对应**两种loss**：观测值的重建损失（loss of ORT）和人工缺失插值损失（loss of MIT），设计这两种loss的目的在于：MIT 被用于迫使模型尽可能准确地预测缺失值，而 ORT 则被利用来确保模型收敛于观察到的数据分布（MIT is utilized to force the model to predict missing values as accurately as possible, and ORT is leveraged to ensure that the model converges to the distribution of observed data.）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cc4ec",
   "metadata": {},
   "source": [
    "由此可知，val\\_X\\_ori为原始数据，只带有自然缺失，而val\\_X则为随机掩码后的数据，带有自然缺失和人工缺失，目的是计算模型针对人工缺失值给出的预测值与真实值的MAE，从而挑选模型。test\\_X\\_ori和test\\_X的关系也是这样，test\\_X\\_ori用于计算最后训练得到的模型对于人工缺失值的预测性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f24e22",
   "metadata": {},
   "source": [
    "*   所有模型统一超参数搜索，早停策略一致\n",
    "*   提出两种任务联合优化（ORT + MIT），在训练中模拟缺失值以增强泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b4acf",
   "metadata": {},
   "source": [
    "### 评价方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133939bb",
   "metadata": {},
   "source": [
    "*   MAE（平均绝对误差）\n",
    "*   RMSE（均方根误差）\n",
    "*   MRE（平均相对误差）\n",
    "*   下游任务：ROC-AUC / PR-AUC / F1 score（PhysioNet 数据上）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c0ace",
   "metadata": {},
   "source": [
    "### 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5073a7e",
   "metadata": {},
   "source": [
    "*   SAITS 在所有数据集和不同缺失率下都取得最佳性能，提升幅度明显（如 MAE 可下降 11%+）\n",
    "*   SAITS 在训练效率上优于 BRITS 和 VAE 类模型\n",
    "*   Ablation 实验证明 DMSA（对角 Mask）和权重组合机制对性能至关重要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d059ead",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c97e78",
   "metadata": {},
   "source": [
    "## 3️⃣ 文章亮点思考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6baaa6",
   "metadata": {},
   "source": [
    "> **优点：**\n",
    ">\n",
    "> *   首次将 Transformer 中自注意力机制以非递归方式用于缺失值填补，并设计对角 Mask 解决信息泄漏问题\n",
    "> *   联合训练框架（ORT+MIT）合理平衡填补准确性与重建能力，理论分析充分\n",
    "> *   模型轻量，参数少于 Transformer 和 BRITS，速度快，推广性强\n",
    "> *   代码开源，实验设置严谨，结果具备可复现性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6b83a",
   "metadata": {},
   "source": [
    "> **缺点：**\n",
    ">\n",
    "> *   主要关注 MCAR 类型的缺失，未深入研究 MNAR 情况\n",
    "> *   在部分极端缺失场景下（如 90% 缺失）性能下降趋势未深入分析\n",
    "> *   当前结构是“硬编码”的两层 DMSA，未来是否可更自适应或动态化还有待探讨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33bb29",
   "metadata": {},
   "source": [
    "> **改进建议：**\n",
    ">\n",
    "> *   进一步探索 SAITS 在 MNAR、MAR 情况下的适应能力\n",
    "> *   加入动态层数或 gated attention 控制 DMSA 叠加深度\n",
    "> *   对于带时间戳或不规则采样的时间序列，可引入时间编码增强建模能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a4f2a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77b153",
   "metadata": {},
   "source": [
    "## 4️⃣ 借鉴学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c4523",
   "metadata": {},
   "source": [
    "### 思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c227e27",
   "metadata": {},
   "source": [
    "*   **联合训练思想：** 将 reconstruction 和 imputation 分开训练难以提升填补能力，需加入强制预测缺失值的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01ac0e",
   "metadata": {},
   "source": [
    "*   **对角 Mask：** 有效避免 attention 自己看自己Self-Attention 的关键在于："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17147956",
   "metadata": {},
   "source": [
    "    $$\n",
    "    A = \\text{Softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e01039",
   "metadata": {},
   "source": [
    "    其中 $A \\in \\mathbb{R}^{T \\times T}$ 是注意力权重矩阵，表示时间步 $t_i$ 对其他 $t_j$ 的“关注度”。> 在原始形式中，这个矩阵对角线（i = j）是允许非零的，意味着 $t_i$ 可以看到自己。**对角 Mask 就是将这些对角线上的值强行屏蔽掉（例如设为 $-\\infty$），使得 softmax 后其权重为 0：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d4b36",
   "metadata": {},
   "source": [
    "    $$\n",
    "    [\\text{DiagMask}(x)]_{i,j} =\n",
    "    \\begin{cases}\n",
    "    -\\infty & \\text{if } i = j \\\\\n",
    "    x_{i,j} & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2ef0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e150b024",
   "metadata": {},
   "source": [
    "    $$\n",
    "    \\text{DMSA}(Q, K, V) = \\text{Softmax}(\\text{DiagMask}(QK^T / \\sqrt{d_k})) \\cdot V\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a56864",
   "metadata": {},
   "source": [
    "    这样就能 **强制模型不能“用自己预测自己”**，只允许参考其他时间点的信息来填补缺失值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50e609",
   "metadata": {},
   "source": [
    "*   **双阶段建模+动态权重融合：** 利用两个 DMSA 块生成不同层次表示，最后加权组合更健壮。**“双阶段建模 + 动态权重融合”**，是指 SAITS 中利用两个不同的自注意力模块（DMSA Blocks）分别学习不同的特征表示，然后通过一个可学习的权重机制，将这两个表示动态融合，得到最终的缺失值预测。***## 🎯 为什么要“双阶段建模”？单个 Transformer 层可能学习不到足够复杂的关系，尤其在时间序列中，缺失值往往需要 **同时建模时间依赖和变量之间的相关性**。所以作者采用了两个 DMSA（Diagonally Masked Self-Attention）Block："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f452355",
   "metadata": {},
   "source": [
    "    1.  **第一阶段（第一 DMSA Block）**：从原始输入中建模特征间关系，并初步填补缺失值。\n",
    "    2.  **第二阶段（第二 DMSA Block）**：接收上一步填补后的数据，进一步优化填补结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5c56",
   "metadata": {},
   "source": [
    "    ***## 🔁 那为什么还要“动态权重融合”？这涉及一个非常重要的现实问题：> 第二阶段的填补并不总比第一阶段好，有时甚至退化。因此作者没有直接使用第二阶段的结果，而是**引入一个加权融合机制**，动态决定每个位置到底该“更信第一阶段”还是“更信第二阶段”。### 📐 公式结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6fad5",
   "metadata": {},
   "source": [
    "    $$\n",
    "    \\tilde{X}_3 = (1 - \\eta) \\odot \\tilde{X}_1 + \\eta \\odot \\tilde{X}_2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a6a6d",
   "metadata": {},
   "source": [
    "    其中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4577b",
   "metadata": {},
   "source": [
    "    *   $\\tilde{X}_1$ ：第一阶段填补结果（Learned Representation 1）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ffb78",
   "metadata": {},
   "source": [
    "    *   $\\tilde{X}_2$ ：第二阶段填补结果（Learned Representation 2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8592c",
   "metadata": {},
   "source": [
    "    *   $\\eta \\in (0,1)^{T \\times D}$ ：每个位置的可学习融合权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d34c",
   "metadata": {},
   "source": [
    "    *   $\\tilde{X}_3$ ：融合后最终的填补结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a0bf5",
   "metadata": {},
   "source": [
    "    注意：这个权重 $\\eta$ 是通过 self-attention 的 attention map 和缺失 mask 一起喂进一个小网络算出来的。***## 🎬 类比理解（打补丁）：你修一张照片，第一遍你大概补好了（模糊了背景噪点），然后你再细修了一遍（锐化人脸）。\\\n",
    "    最后你要判断：每一块地方到底用第一遍的模糊好，还是第二遍的锐化好？\\\n",
    "    于是你做了一个**加权融合**，每一块自动决定用哪个版本权重高一点。***## 📊 动态融合机制的意义："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca26d9",
   "metadata": {},
   "source": [
    "    | 优点   | 说明                          |\n",
    "    | ---- | --------------------------- |\n",
    "    | 适应性强 | 每个时间点每个变量都有自己的一对融合权重        |\n",
    "    | 避免退化 | 第二阶段可能并不总是更好，所以不能盲目信任       |\n",
    "    | 参数高效 | 融合只用一层线性+Sigmoid 生成权重，计算量很小 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c14f0",
   "metadata": {},
   "source": [
    "\n",
    "### 工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e228f",
   "metadata": {},
   "source": [
    "*   PyTorch\n",
    "*   GitHub: <https://github.com/WenjieDu/SAITS>\n",
    "*   四个数据集的标准处理脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147811bc",
   "metadata": {},
   "source": [
    "### 问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd7e70",
   "metadata": {},
   "source": [
    "*   SAITS 的架构是否能适配实时流数据填补？\n",
    "*   对比 Diffusion 模型（如CSDI）在不规则采样和大缺失率下是否更具优势？\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
